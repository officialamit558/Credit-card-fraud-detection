# -*- coding: utf-8 -*-
"""Credit_Card_Fraud_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uayBTMrupSkL-Dk92h-ukrW0SG1OsN-T
"""



"""# Import Relevent Library"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
# %matplotlib inline

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

import sklearn
from sklearn import metrics
from sklearn.metrics import roc_auc_score , roc_curve , auc
from sklearn.metrics import classification_report , confusion_matrix
from sklearn.metrics import average_precision_score , precision_recall_curve

from sklearn.model_selection import train_test_split , StratifiedKFold , GridSearchCV , RandomizedSearchCV

from sklearn.linear_model import Ridge , Lasso , LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier

import xgboost as xgb
from xgboost import XGBClassifier
from xgboost import plot_importance

"""# Exploratory Data Analysis"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

#! chmod 600 /root/.kaggle/kaggle.json

! kaggle datasets download -d mlg-ulb/creditcardfraud

import zipfile
zip_ref = zipfile.ZipFile('/content/creditcardfraud.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

df = pd.read_csv("/content/creditcard.csv")
df.head()

df.shape

df.info()

df.describe()

ans = df["Class"].value_counts()
ans

ans = list(ans)

ans[0]/df["Class"].count()*100,ans[1]/df["Class"].count()*100

cor = df.corr()
cor

import seaborn as sns

plt.figure(figsize = (24 , 18))
sns.heatmap(cor , cmap = "coolwarm" , annot = True)
plt.show()

df.head(3)

# Feature engineering

delta_time = pd.to_timedelta(df["Time"] , unit = "s")

df["Time_Day"] = (delta_time.dt.components.days).astype(int)
df["Time_Hour"] = (delta_time.dt.components.hours).astype(int)
df["Time_Minute"] = (delta_time.dt.components.minutes).astype(int)

df.head(4)

df.drop("Time" , axis = 1 , inplace= True)

df.head(0)

df.drop(["Time_Day","Time_Minute"] , axis = 1 , inplace = True)

df.head(5)

# Train test and split the datasets for the model trainig and evaluation

y = df["Class"]
x = df.drop("Class" , axis = 1)

y.shape

x.shape

X_train , X_test , y_train , y_test = train_test_split(x, y , test_size = 0.2 , random_state =100)

X_train.shape , y_train.shape , X_test.shape , y_test.shape

"""# Model Building"""

# Create the dataframe to store result
df_result = pd.DataFrame(columns = ["Methodology" ,"Model","Accuracy" ,"roc_value" ,"threshold"])

#Create the common function to plot confusion matrix

def plot_confusion_matrix(y_test , pred_test):
  cm = confusion_matrix(y_test , pred_test)
  plt.clf()
  plt.imshow(cm , interpolation="nearest" , cmap=plt.cm.Accent)
  categoryNames = ['Non-Fraudalent' , 'Fraudalent']
  plt.title("Confusion Matrix - Test Data")
  plt.ylabel("True label")
  plt.xlabel("Predicted label")
  ticks = np.arange(len(categoryNames))
  plt.xticks(ticks , categoryNames , rotation = 45)
  plt.yticks(ticks , categoryNames)
  s = [["TN" ,"FP"] , ["FN" , "TP"]]

  for i in range(2):
    for j in range(2):
      plt.text(j,i , str(s[i][j])+" = "+str(cm[i][j]) , fontsize=12)
  plt.show()

"""# Build Logistic Model"""

#create the common function to fit and predict on the logistics regression model for L1 and L2 Norms

def buildAndRunLogisticModel(df_result , Methodology , X_train , y_train , X_test , y_test):

  from sklearn import linear_model
  from sklearn.model_selection import KFold

  num_c = list(np.power(10.0 ,np.arange(-10,10)))
  cv_num = KFold(n_splits = 10 , shuffle = True , random_state=42)

  searchcv_l2 = linear_model.LogisticRegressionCV(
      Cs = num_c
      ,penalty = 'l2'
      ,scoring='roc_auc'
      ,cv = cv_num
      ,random_state = 42
      ,max_iter = 10000
      ,fit_intercept= True
      ,solver = 'liblinear'
      ,tol = 10
  )

  searchcv_l1 = linear_model.LogisticRegressionCV(
      Cs = num_c,
      penalty = 'l1',
      scoring = 'roc_auc',
      cv = cv_num,
      random_state = 42,
      max_iter = 10000,
      fit_intercept= True,
      solver = 'liblinear',

  )

  searchcv_l1.fit(X_train , y_train)
  searchcv_l2.fit(X_train , y_train)
  print("Max accuracy for l1:", searchcv_l1.score(X_test, y_test).mean(axis=0).max())
  print("Max accuracy for l2:", searchcv_l2.score(X_test, y_test).mean(axis=0).max())


  print("Parameter for l1 regularization ")
  print(searchcv_l1.coef_)
  print(searchcv_l1.score_)
  print(searchcv_l1.intercept_)

  print("Parametr for the l2 REGULARIZATION")
  print(searchcv_l2.score_)
  print(searchcv_l2.intercept_)
  print(searchcv_l2.coef_)

  # find the predicted value.
  y_pred_l1 = searchcv_l1.predict(X_test)
  y_pred_l2 = searchcv_l2.predict(X_test)

  # find the predicted probabilites

  y_pred_prob_l1 = searchcv_l1.predict_proba(X_test)[:,1]
  y_pred_prob_l2 = searchcv_l2.predict_proba(X_test)[:,1]

  # Accuracy of l2/l1 models
  Accuracy_l2 = metrics.accuracy_score(y_pred_prob_l2 ,y_true = y_test)
  Accuracy_l1 = metrics.accuracy_score(y_pred_prob_l1 , y_true = y_test)

  print("Accuracy of the logistics model of l1 regularization : {0}".format(Accuracy_l1))
  print("Confusion Matrix")
  plot_confusion_matrix(y_test , y_pred_l1)
  print("Classification report ")
  print(classification_report(y_test , y_pred_l1))

  print("Accuracy of the logistics model of l2 reularization :{0}".format(Accuracy_l2))
  print("Confusion Matrix")
  plot_confusion_matrix(y_test , y_pred_l2)
  print("Classification report ")
  print(classification_report(y_test , y_pred_l2))


  l2_roc_value = roc_auc_score(y_test , y_pred_prob_l2)
  print(("l2 roc_value: {0}" .format(l2_roc_value)))
  fpr , tpr , thresholds = metrics.roc_auc(y_test , y_pred_prob_l2)
  threshold = thresholds[np.argmax(tpr-fpr)]
  print("l2 threshold: {0}" .format(threshold))

  roc_auc = metrics.auc(fpr , tpr)
  print("roc for the test dataset" , "{:.1%}") .format(roc_auc)
  plt.plot(fpr , tpr , label="Test , auc"+str(roc_auc))
  plt.legend(loc=4)
  plt.show()

  df_result = df_result.append(pd.DataFrame({"Methodology":Methodology , "Model":"Logistic_Regression With L2 Regularization ","Accuracy":Accuracy_l2 , "roc_value": l2_roc_value , "threshold": threshold}))

  l1_roc_value = roc_auc_score(y_test , y_pred_prob_l1)
  print(("l2 roc_value: {0}" .format(l1_roc_value)))
  fpr , tpr , thresholds = metrics.roc_auc(y_test , y_pred_prob_l1)
  threshold = thresholds[np.argmax(tpr-fpr)]
  print("l1 threshold: {0}" .format(threshold))

  roc_auc = metrics.auc(fpr , tpr)
  print("roc for the test dataset" , "{:.1%}") .format(roc_auc)
  plt.plot(fpr , tpr , label="Test , auc"+str(roc_auc))
  plt.legend(loc=4)
  plt.show()

  df_result = df_result.append(pd.DataFrame({"Methodology":Methodology , "Model":"Logistic_Regression With L1 Regularization ","Accuracy":Accuracy_l1 , "roc_value": l1_roc_value , "threshold": threshold}, index = [0]) , ignore_index = True)
  return df_result

"""# KNN Model Building"""

def buildAndRunKNNModel(df_result , Methodology , X_train , y_train , X_test , y_test):

  knn = KNeighborsClassifier(n_neighbors=5 , n_jobs=16)
  knn.fit(X_train , y_train)
  score = knn.score(X_test , y_test)
  print(score)

  # Accuracy
  y_pred = knn.predict(X_test)
  Knn_Accuracy  = metrics.accuracy_score(y_pred = y_pred , y_true = y_test)
  print("Confusion Matrix")
  plot_confusion_matrix(y_test , y_pred)
  print("classification Report")

  knn_probs = knn.predict_probs(X_test)[: , 1]

  #Calculate roc auc
  knn_roc_value = roc_auc_score(y_test , knn_probs)
  print("KNN roc_auc: {0}" .format(knn_roc_value))
  fpr , tpr , thresholds = metrics.roc_curve(y_test , knn_probs)
  threshold = thresholds[np.argmax(tpr - fpr)]
  print("KNN thresholds: {0}" .format(threshold))

  roc_auc = metrics.auc(fpr , tpr)
  print("ROC for the dataset" , "{:.1%}".format(roc_auc))
  plt.plot(fpr , tpr , label = "Test , Auc"+str(roc_auc))
  plt.legend(loc=4)
  plt.show()

  df_result = df_result.append(pd.DataFrame({"Methodology":Methodology , "Model":"KNN ","Accuracy":score , "roc_value": knn_roc_value , "threshold": threshold} , index = [0]) , ignore_index = True)

  return df_result

"""# Decision Tree Model"""

def buildAndRunDecisionTreeModels(df_result , Methodology , X_train , y_train , X_test , y_test):
  criteria = ["gini" , "entropy"]
  scores = {}

  for c in criteria:
    dt = DecisionTreeClassifier(criteria = c , random_state = 42)
    dt.fit(X_train , y_train)
    y_pred = dt.predict(X_test)
    test_score = dt.score(X_test , y_test)
    tree_preds = dt.predict_proba(X_test)[:,1]
    tree_roc_value = roc_auc_score(y_test , tree_preds)
    scores = test_score
    print(c = "score; {0}".format(test_score))
    print("Confusion Matrix")
    plot_confusion_matrix(y_test , y_pred)
    print("Classification report")
    print(classification_report(y_test , y_pred))
    print(c + "tree_roc_value: {0}".format(tree_roc_value))
    fpr , tpr , thresholds = metrics.roc_curve(y_test , tree_preds)
    threshold = thresholds[np.argmax(tpr - fpr)]
    print("Tree threshold: {0}".format(threshold))
    roc_auc = metrics.auc(fpr , tpr)
    print("ROC for the dataset" , "{:.1%}".format(roc_auc))
    plt.plot(fpr , tpr , label="Test , auc"+str(roc_auc))
    plt.legend(loc = 4)
    plt.show()

    df_result = df_result.append(pd.DataFrame({"Methodology":Methodology , "Model":"Tree Model with {0} criteria ".format(c),"Accuracy":test_score , "roc_value": tree_roc_value , "threshold": threshold} , index = [0]) , ignore_index = True)

  return df_result

"""# Random Forest Model"""

def buildAndRunRadomeForestModels(df_result , Methodology , X_train , y_train , X_test , y_test):
  #Create the model with 100 Decision Tree

  RF_model = RandomForestClassifier(n_estimators=100 , bootstrap = True , max_features="sqrt" ,  random_state=42)

  # Fit the Model

  RF_model.fit(X_train , y_train)
  RF_test_score = RF_model.score(X_test , y_test)
  RF_model.predict(X_test)

  print("Model Accuracy: {0}".format(RF_test_score))

  #Actual Class prediction

  rf_predictions = RF_model.predict(X_test)

  print("Confusion Matrix")
  plot_confusion_matrix(y_test , rf_predictions)
  print("classification report")
  print(classification_report(y_test , rf_predictions))

  # Probabilities for each class

  rf_probs = RF_model.predict_probs(X_test)[:,1]

  # Calculate the roc and auc

  roc_value = roc_auc_score(y_test , rf_probs)

  print("Random forest ROC value: {0}".format(roc_value))

  fpr , tpr , thresholds = metrics.roc_curve(y_test , rf_probs)
  threshold = thresholds[np.argmax(tpr - fpr)]
  print("RF threshold: {0}".format(threshold))
  roc_auc = metrics.auc(fpr , tpr)
  print("ROC for the test dataset" , "{:.1%}".format(roc_auc))
  plt.plot(fpr , tpr , label= "Test , auc="+str(roc_auc))
  plt.legend(loc=4)

  df_result = df_result.append(pd.DataFrame({"Methodology":Methodology , "Model":"Random Forest","Accuracy":RF_test_score , "roc_value": roc_value , "threshold": threshold} , index = [0]),ignore_index=True)

  return df_result

"""# XGBoost Model"""

def buildAndRunXGBoostModel(df_result , Methodology , X_train , y_train , X_test , y_test):

  # Evaluate the XGBoost Model
  XGBmodel = XGBClassifier(random_state = 42)
  XGBmodel.fit(X_train , y_test)
  y_pred = XGBmodel.predict(X_test)

  XGB_test_score = XGBmodel.score(X_test , y_test)
  print("Model Accuracy: {0}".format(XGB_test_score))

  print("Confusion Matrix")
  plot_confusion_matrix(y_test , y_pred)
  print("Classification Report")
  print(classification_report(y_test , y_pred))

  # Probabilities for each class
  XGB_probs = XGBmodel.predict_proba(X_test)[:, 1]

  # Calculate the roc and auc
  XGB_roc_value = roc_auc_score(y_test , XGB_probs)

  print("XGBoost roc_value: {0}" .format(XGB_roc_value))
  fpr , tpr , thresholds = metrics.roc_curve(y_test , XGB_probs)
  threshold = thresholds[np.argmax(tpr-fpr)]
  print("XGBoost threshold: {0}".format(threshold))
  roc_auc = metrics.auc(fpr , tpr)
  print("ROC for the test dataset","{:.1%}".format(roc_auc))
  plt.plot(fpr , tpr , label = "Test , auc="+str(roc_auc))
  plt.legend(loc=4)
  plt.show()

  df_result = df_result.append(pd.DataFrame({"Methodology":Methodology , "Model":"XGBoost ","Accuracy":XGB_test_score , "roc_value": XGB_roc_value , "threshold": threshold} , index = [0]),ignore_index = True)

  return df_result

"""# SVM Model"""

def buildAndRunSVMModels(df_result , Methodology , X_train , y_train , X_test , y_test):

  # Evaluate SVM model with sigmoid kernal model
  from sklearn.svm import SVM
  from sklearn.metrics import accuracy_score
  from sklearn.metrics import roc_auc_score

  clf = SVM(kernel = "sigmoid",random_state =42)
  clf.fit(X_train , y_train)
  y_pred_SVM = clf.predict(X_test)
  SVM_Score = accuracy_score(y_test , y_pred_SVM)
  print("accuracy_score: {0}".format(SVM_Score))
  print("Confusion Metrics")
  plot_confusion_metrix(y_test , y_pred_SVM)
  print("classification Report")
  print(classification_report(y_test , y_pred_SVM))

  #Run classifier
  classifier = SVM(kernel = 'sigmoid' , probability = True)
  svm_probs = classifier.fit(X_train , y_train).predict_proba(X_test)[: , 1]

  # Calculate roc and  auc
  roc_value = roc_auc_score(y_test , svm_probs)

  print("SVM roc_value: {0}".format(roc_value))
  fpr , tpr  , thresholds = metrics.roc_curve(y_test , svm_probs)
  threshold = thresholds[np.argmax(tpr - fpr)]
  print("SVM threshold: {0}".format(threshold))
  roc_auc = metrics.auc(fpr , tpr)
  plt.plot(fpr , tpr , label = "Test , auc="+str(roc_auc))
  plt.legend(loc = 4)
  plt.show()

  df_result = df_result.append(pd.DataFram({"methodology":Methodology , "Model":"SVM" , "Accuracy":SVM_Score , "roc_value":roc_value , "threshold":threshold} , index=[0]) , ignore_index = True)
  return df_result

"""# Perform the cross vaidation with RepeatedKFold"""

# RepetedKFold and StratifiedKFold

from sklearn.model_selection import RepeatedKFold
rkf = RepeatedKFold(n_splits= 10 , n_repeats = 10 , random_state = None)
for train_index , test_index in rkf.split(x,y):
  print("TRAIN" , train_index , "TEST",test_index)
  X_train_cv , X_test_cv = x.iloc[train_index],x.iloc[test_index]
  y_train_cv , y_test_cv = y.iloc[train_index],y.iloc[test_index]

#Run Logistic Regression with L1 and L2 regularization
print("Logistic Regression with L! and L2 Regularization")
start_time = time.time()
df_result = buildAndRunLogisticModel(df_result , "RepeatedKFold Cross Validation" , X_train_cv , y_train_cv , X_test_cv , y_test_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

# Run KNN Model
print("KNN Model")
start_time = time.time()
df_result = buildAndRunKNNModel(df_result , "RepeatedKFold Cross Validation" , X_train_cv , y_train_cv , X_test_cv , y_test_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )


# Decison Tree Model
print("Decision Tree Model")
start_time = time.time()
df_result = buildAndRunDecisionTreeModels(df_result , "RepeatedKFold Cross Validation" , X_train_cv , y_train_cv , X_test_cv , y_test_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

# Random Forest Model
print("Random Forest model")
start_time = time.time()
df_result = buildAndRunRadomeForestModels(df_result , "RepeatedKFold Cross Validation" , X_train_cv , y_train_cv , X_test_cv , y_test_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

# XGBoost Model
print("XGBoost Model")
start_time = time.time()
df_result = buildAndRunXGBoostModel(df_result , "RepeatedKFold Cross Validation" , X_train_cv , y_train_cv , X_test_cv , y_test_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

#SVM Model
print('SVM Model')
start_time = time.time()
df_result = buildAndRunSVMModels(df_result , "RepeatedKFold Cross Validation" , X_train_cv , y_train_cv , X_test_cv , y_test_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

df_result

# Result for cross validation with StratifiedKFold
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5 , random_state = 42)
for train_index , test_index in skf.split(x,y):
  print('TRAIN:',train_index , "TEST:",test_index)
  X_train_SKF_cv , X_test_SKF_cv = x.iloc[train_index] , x.iloc[test_index]
  y_train_SKF_cv , y_test_SKF_cv = y.iloc[train_index] , y.iloc[test_index]

# Run Logistic Regression with L1 and L2 Regularization
print("Logistic regression with L1 and L2 Regularization")
start_time = time.time()
df_result = buildAndRunLogisticModel(df_result , "StratifiedKFold Cross Validation" , X_train_SKF_cv , y_train_SKF_cv , X_test_SKF_cv , y_test_SKF_cv)
print("Time Taken by Model: ---%s seconds ---"(time.time() - start_time))
print(' - '*60)

# Run KNN Model
print("KNN Model")
start_time = time.time()
df_result = buildAndRunKNNModel(df_result , "StratifiedKFold Cross Validation" , X_train_SKF_cv , y_train_SKF_cv , X_test_SKF_cv , y_test_SKF_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

# Decison Tree Model
print("Decision Tree Model")
start_time = time.time()
df_result = buildAndRunDecisionTreeModels(df_result , "StratifiedKFold Cross Validation" , X_train_SKF_cv , y_train_SKF_cv , X_test_SKF_cv , y_test_SKF_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

# Random Forest Model
print("Random Forest model")
start_time = time.time()
df_result = buildAndRunRadomeForestModels(df_result , "StratifiedKFold Cross Validation" , X_train_SKF_cv , y_train_SKF_cv , X_test_SKF_cv , y_test_SKF_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

# XGBoost Model
print("XGBoost Model")
start_time = time.time()
df_result = buildAndRunXGBoostModel(df_result , "StratifiedKFold Cross Validation" , X_train_SKF_cv , y_train_SKF_cv , X_test_SKF_cv , y_test_SKF_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

#SVM Model
print('SVM Model')
start_time = time.time()
df_result = buildAndRunSVMModels(df_result , "StratifiedKFold Cross Validation" , X_train_SKF_cv , y_train_SKF_cv , X_test_SKF_cv , y_test_SKF_cv)
print("Time taken by Model: ----%s seconds ---" % (time.time() - start_time))
print(' _ '*60 )

df_result

"""# Results for cross validation with StratifiedKFold"""

from sklearn import linear_model
from sklearn.model_selection import KFold

num_c = list(np.power(10 , np.arange(-10 , 10)))
cv_num = KFold(n_splits = 10 , shuffle = True , random_state = 42)

clf = linear_model.LogisticRegressionCV(
    Cs = num_c,
    ,penalty = 'l2'
    ,scoring='roc_auc'
    ,cv=cv_num
    ,random_state = 42
    ,max_iter = 10000
    ,fit_intercept = True
    ,solver = 'liblinear'
    ,tol = 10
)

clf.fit(X_train_SKF_cv , y_train_SKF_cv)
print('Max auc_roc for l1:', clf.score[X_train_SKF_cv , y_train_SKF_cv].mean(axis=0).max())

print(clf.coef_)
print(clf.intercept_)
print(clf.scores_)

y_pred_l2 = clf.predict(X_test)

y_pred_proba_l2 = clf.predict_proba(X_test)[: , 1]

Accuracy_l2 = metrics.accuracy_score(y_pred = y_pred_l2 , y_true = y_test)

print("Accuracy of logistics model with regularization " .format(Accuracy_l2))

from sklearn.metrics import roc_auc_score

l2_roc_value = roc_auc_score(y_test , y_pred_prob_l2)
fpr , tpr , thresholds = metrics.roc_curve(y_test , y_pred_prob_l2)
threshold = thresholds[np.argmax(tpr - fpr)]

# Checking for the coefficient values
clf.coef_

coefficient = pd.concat([pd.DataFrame(x.colums) , pd.DataFrame(np.transpose(clf.coef_))] , axis = 1)
coefficient.coulms = ['Featrue' , 'Importance Coefficient']

# Deu to low computing power in our computer , Take very much time during training .













